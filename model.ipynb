{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2cea13d8",
   "metadata": {},
   "source": [
    "# Email Sentiments Analysis using Vader and Naive Bayes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f43954a",
   "metadata": {},
   "source": [
    "## Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3abc3f56",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\dgoya\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\dgoya\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package vader_lexicon to\n",
      "[nltk_data]     C:\\Users\\dgoya\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package vader_lexicon is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "import re\n",
    "import csv\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "from nltk.classify import NaiveBayesClassifier\n",
    "\n",
    "# download NLTK resources if not already downloaded\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('vader_lexicon')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ad114ea",
   "metadata": {},
   "source": [
    "## Load dataset file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f6f9d6d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load email data from CSV file\n",
    "data_path = 'emails.csv'\n",
    "messages = []\n",
    "with open(data_path, 'r') as csvfile:\n",
    "    reader = csv.reader(csvfile)\n",
    "    for row in reader:\n",
    "        messages.append(row[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84e6cd61",
   "metadata": {},
   "source": [
    "## Cleaning the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8fc35e19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to clean text by removing hyperlinks and stopwords\n",
    "def clean_text(text):\n",
    "    text = re.sub(r'http\\S+', '', text)  # remove hyperlinks\n",
    "    text = re.sub(r'\\b\\w{1,3}\\b', '', text)  # remove short words\n",
    "    text = re.sub(r'[^\\w\\s]', '', text)  # remove punctuation\n",
    "    text = text.lower()  # convert to lowercase\n",
    "    tokens = word_tokenize(text)  # tokenize text\n",
    "    stop_words = set(stopwords.words('english'))  # get stop words\n",
    "    filtered_tokens = [token for token in tokens if token not in stop_words]  # remove stop words\n",
    "    cleaned_text = ' '.join(filtered_tokens)  # join tokens back into string\n",
    "    return cleaned_text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "287820c6",
   "metadata": {},
   "source": [
    "## Initialize VADER sentiment analyzer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9d78d260",
   "metadata": {},
   "outputs": [],
   "source": [
    "analyzer = SentimentIntensityAnalyzer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4d7b9d9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# clean email messages and extract features for Naive Bayes classifier\n",
    "featuresets = []\n",
    "for message in messages:\n",
    "    cleaned_message = clean_text(message)\n",
    "    scores = analyzer.polarity_scores(cleaned_message)\n",
    "    features = {\n",
    "        'positive_score': scores['pos'],\n",
    "        'negative_score': scores['neg'],\n",
    "        'neutral_score': scores['neu'],\n",
    "        'compound_score': scores['compound']\n",
    "    }\n",
    "    featuresets.append((features, 'positive' if scores['compound'] >= 0.05 else 'negative' if scores['compound'] <= -0.05 else 'neutral'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "204714b0",
   "metadata": {},
   "source": [
    "## Train Naive Bayes Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1c132031",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train Naive Bayes classifier on featuresets\n",
    "classifier = NaiveBayesClassifier.train(featuresets)\n",
    "\n",
    "# test classifier on new email message\n",
    "new_message = \"Hi John, just wanted to touch base with you about the project we discussed last week. i also have surprised for you too\"\n",
    "cleaned_new_message = clean_text(new_message)\n",
    "scores = analyzer.polarity_scores(cleaned_new_message)\n",
    "features = {\n",
    "    'positive_score': scores['pos'],\n",
    "    'negative_score': scores['neg'],\n",
    "    'neutral_score': scores['neu'],\n",
    "    'compound_score': scores['compound']\n",
    "}\n",
    "classification = classifier.classify(features)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1675133",
   "metadata": {},
   "source": [
    "## print sentiment scores and classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5bb8971d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Positive score: 0.174\n",
      "Negative score: 0.0\n",
      "Neutral score: 0.826\n",
      "Compound score: 0.2263\n",
      "Naive Bayes classification: positive\n"
     ]
    }
   ],
   "source": [
    "print(\"Positive score:\", scores['pos'])\n",
    "print(\"Negative score:\", scores['neg'])\n",
    "print(\"Neutral score:\", scores['neu'])\n",
    "print(\"Compound score:\", scores['compound'])\n",
    "print(\"Naive Bayes classification:\", classification)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18c8c09b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e8bdcb6c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\dgoya\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\dgoya\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package vader_lexicon to\n",
      "[nltk_data]     C:\\Users\\dgoya\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package vader_lexicon is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix:\n",
      " [[162   0   2]\n",
      " [  3 436   0]\n",
      " [  9   2 501]]\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.93      0.99      0.96       164\n",
      "     neutral       1.00      0.99      0.99       439\n",
      "    positive       1.00      0.98      0.99       512\n",
      "\n",
      "    accuracy                           0.99      1115\n",
      "   macro avg       0.97      0.99      0.98      1115\n",
      "weighted avg       0.99      0.99      0.99      1115\n",
      "\n",
      "Accuracy: 0.9856502242152466\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "import re\n",
    "import csv\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "from nltk.classify import NaiveBayesClassifier\n",
    "from sklearn.metrics import confusion_matrix, classification_report, accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# download NLTK resources if not already downloaded\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('vader_lexicon')\n",
    "\n",
    "# load email data from CSV file\n",
    "data_path = 'emails.csv'\n",
    "messages = []\n",
    "with open(data_path, 'r') as csvfile:\n",
    "    reader = csv.reader(csvfile)\n",
    "    for row in reader:\n",
    "        messages.append(row[0])\n",
    "\n",
    "# function to clean text by removing hyperlinks and stopwords\n",
    "def clean_text(text):\n",
    "    text = re.sub(r'http\\S+', '', text)  # remove hyperlinks\n",
    "    text = re.sub(r'\\b\\w{1,3}\\b', '', text)  # remove short words\n",
    "    text = re.sub(r'[^\\w\\s]', '', text)  # remove punctuation\n",
    "    text = text.lower()  # convert to lowercase\n",
    "    tokens = word_tokenize(text)  # tokenize text\n",
    "    stop_words = set(stopwords.words('english'))  # get stop words\n",
    "    filtered_tokens = [token for token in tokens if token not in stop_words]  # remove stop words\n",
    "    cleaned_text = ' '.join(filtered_tokens)  # join tokens back into string\n",
    "    return cleaned_text\n",
    "\n",
    "analyzer = SentimentIntensityAnalyzer()\n",
    "\n",
    "# clean email messages and extract features for Naive Bayes classifier\n",
    "featuresets = []\n",
    "labels = []\n",
    "for message in messages:\n",
    "    cleaned_message = clean_text(message)\n",
    "    scores = analyzer.polarity_scores(cleaned_message)\n",
    "    features = {\n",
    "        'positive_score': scores['pos'],\n",
    "        'negative_score': scores['neg'],\n",
    "        'neutral_score': scores['neu'],\n",
    "        'compound_score': scores['compound']\n",
    "    }\n",
    "    featuresets.append(features)\n",
    "    if scores['compound'] >= 0.05:\n",
    "        labels.append('positive')\n",
    "    elif scores['compound'] <= -0.05:\n",
    "        labels.append('negative')\n",
    "    else:\n",
    "        labels.append('neutral')\n",
    "\n",
    "# split dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(featuresets, labels, test_size=0.2, random_state=42)\n",
    "\n",
    "# train Naive Bayes classifier on training set\n",
    "classifier = NaiveBayesClassifier.train(list(zip(X_train, y_train)))\n",
    "\n",
    "# test classifier on testing set\n",
    "y_pred = classifier.classify_many(X_test)\n",
    "\n",
    "# print evaluation metrics\n",
    "print('Confusion Matrix:\\n', confusion_matrix(y_test, y_pred))\n",
    "print('Classification Report:\\n', classification_report(y_test, y_pred))\n",
    "print('Accuracy:', accuracy_score(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e6be3bc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
